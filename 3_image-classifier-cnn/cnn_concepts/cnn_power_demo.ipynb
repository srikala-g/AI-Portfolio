{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Power of CNN Classifiers: A Comprehensive Demonstration\n",
        "\n",
        "This notebook demonstrates the remarkable capabilities of Convolutional Neural Networks (CNNs) across multiple domains and applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10, mnist, fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. CNN Architecture Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN architectures defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_simple_cnn(input_shape, num_classes):\n",
        "    \"\"\"Simple CNN for comparison\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_advanced_cnn(input_shape, num_classes):\n",
        "    \"\"\"Advanced CNN with dropout and batch normalization\"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"CNN architectures defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multi-Dataset CNN Performance Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting CNN Power Demonstration...\n",
            "Loading datasets...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 0us/step\n",
            "\n",
            "==================================================\n",
            "Training CNN on MNIST\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.9025 - loss: 0.3245 - val_accuracy: 0.2556 - val_loss: 3.3911\n",
            "Epoch 2/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 45ms/step - accuracy: 0.9710 - loss: 0.0930 - val_accuracy: 0.9833 - val_loss: 0.0569\n",
            "Epoch 3/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 46ms/step - accuracy: 0.9797 - loss: 0.0644 - val_accuracy: 0.9877 - val_loss: 0.0430\n",
            "Epoch 4/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.9831 - loss: 0.0539 - val_accuracy: 0.9914 - val_loss: 0.0319\n",
            "Epoch 5/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.9857 - loss: 0.0450 - val_accuracy: 0.9908 - val_loss: 0.0303\n",
            "\n",
            "MNIST Test Accuracy: 0.9945\n",
            "\n",
            "==================================================\n",
            "Training CNN on Fashion-MNIST\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.7469 - loss: 0.7257 - val_accuracy: 0.4280 - val_loss: 1.6217\n",
            "Epoch 2/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 47ms/step - accuracy: 0.8452 - loss: 0.4213 - val_accuracy: 0.8836 - val_loss: 0.3226\n",
            "Epoch 3/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 49ms/step - accuracy: 0.8685 - loss: 0.3561 - val_accuracy: 0.8913 - val_loss: 0.2975\n",
            "Epoch 4/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.8830 - loss: 0.3211 - val_accuracy: 0.8959 - val_loss: 0.2849\n",
            "Epoch 5/5\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 46ms/step - accuracy: 0.8909 - loss: 0.2969 - val_accuracy: 0.9046 - val_loss: 0.2598\n",
            "\n",
            "Fashion-MNIST Test Accuracy: 0.8971\n",
            "\n",
            "==================================================\n",
            "Training CNN on CIFAR-10\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 63ms/step - accuracy: 0.3772 - loss: 1.9319 - val_accuracy: 0.1952 - val_loss: 2.7775\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.5342 - loss: 1.3090 - val_accuracy: 0.5541 - val_loss: 1.2308\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 65ms/step - accuracy: 0.5977 - loss: 1.1206 - val_accuracy: 0.6231 - val_loss: 1.0622\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step - accuracy: 0.6390 - loss: 1.0091 - val_accuracy: 0.6546 - val_loss: 0.9799\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.6763 - loss: 0.9151 - val_accuracy: 0.5775 - val_loss: 1.2329\n",
            "\n",
            "CIFAR-10 Test Accuracy: 0.5759\n"
          ]
        }
      ],
      "source": [
        "def demonstrate_cnn_power():\n",
        "    \"\"\"Demonstrate CNN power across multiple datasets\"\"\"\n",
        "    \n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    \n",
        "    # MNIST (Handwritten digits)\n",
        "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "    x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "    x_test_mnist = x_test_mnist.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "    y_train_mnist = to_categorical(y_train_mnist, 10)\n",
        "    y_test_mnist = to_categorical(y_test_mnist, 10)\n",
        "    \n",
        "    # Fashion-MNIST (Clothing items)\n",
        "    (x_train_fashion, y_train_fashion), (x_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
        "    x_train_fashion = x_train_fashion.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "    x_test_fashion = x_test_fashion.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "    y_train_fashion = to_categorical(y_train_fashion, 10)\n",
        "    y_test_fashion = to_categorical(y_test_fashion, 10)\n",
        "    \n",
        "    # CIFAR-10 (Natural images)\n",
        "    (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "    x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "    x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "    y_train_cifar = to_categorical(y_train_cifar, 10)\n",
        "    y_test_cifar = to_categorical(y_test_cifar, 10)\n",
        "    \n",
        "    datasets = {\n",
        "        'MNIST': (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist, (28, 28, 1)),\n",
        "        'Fashion-MNIST': (x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion, (28, 28, 1)),\n",
        "        'CIFAR-10': (x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar, (32, 32, 3))\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for dataset_name, (x_train, y_train, x_test, y_test, input_shape) in datasets.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training CNN on {dataset_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        # Create and compile model\n",
        "        model = create_advanced_cnn(input_shape, 10)\n",
        "        model.compile(optimizer='adam',\n",
        "                     loss='categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "        \n",
        "        # Train model (reduced epochs for demo)\n",
        "        history = model.fit(x_train, y_train,\n",
        "                          epochs=5,  # Reduced for demo\n",
        "                          batch_size=128,\n",
        "                          validation_split=0.2,\n",
        "                          verbose=1)\n",
        "        \n",
        "        # Evaluate model\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        \n",
        "        results[dataset_name] = {\n",
        "            'accuracy': test_acc,\n",
        "            'history': history,\n",
        "            'model': model\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n{dataset_name} Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the demonstration\n",
        "print(\"Starting CNN Power Demonstration...\")\n",
        "results = demonstrate_cnn_power()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
